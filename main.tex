\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{P-BOT: Your personal PDF chatbot\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Shubhi Parashar}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Parth Shah}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Manan Salia}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
In response to the challenges posed by digital intelligence in the digital economy, the emergence of Artificial Intelligence Generated Content (AIGC)[1-3] has been noteworthy. AIGC leverages artificial intelligence to facilitate or even replace manual content generation, generating content in accordance with user-provided keywords or specifications. The continuous advancement of large-scale model algorithms has significantly enhanced the capabilities of AIGC, rendering it a promising generative tool that enhances convenience in various aspects of our lives. Positioned as an upstream technology, AIGC possesses limitless potential to support diverse downstream applications. A pivotal transformation is underway in the realm of content creation and knowledge representation due to the widespread adoption of large artificial intelligence models such as ChatGPT[2]. AIGC, driven by these generative large AI algorithms, is at the forefront of this paradigm shift. It empowers users by aiding or even substituting human efforts in generating extensive, top-tier, and remarkably human-like content swiftly and cost-effectively, all hinging on user-provided prompts. 

Generative AI[4] stands as a pivotal component within the Artificial Intelligence Generated Content (AIGC) consortium, playing a significant role in shaping the future of AI-driven interactions. Generative AI facilitates the development of conversational agents and chatbots capable of engaging in dynamic and contextually relevant interactions, thereby enhancing user experiences across a wide range of applications, from customer service to content generation. This innovative technology continues to evolve, pushing the boundaries of what AI can achieve in understanding and generating human language, ultimately paving the way for more intelligent and responsive AI systems.

Leveraging Generative AI represents a strategic advantage, as it enables the integration of advanced tools capable of addressing inquiries pertaining to PDF documents. In the contemporary digital era, libraries have evolved beyond their traditional physical confines to encompass a vast array of online resources tailored to serve their patrons. A widely adopted format for these digital resources is the Portable Document Format (PDF), which facilitates the seamless dissemination of information while preserving the original document's layout and formatting. Consequently, PDFs have gained popularity, particularly for the storage and distribution of academic articles and research reports. Nevertheless, PDFs present a unique challenge within library systems, primarily stemming from their limited interactivity when compared to dynamic web pages and other digital content sources. Unlike more interactive resources, PDFs lack essential features that enable users to conduct in-document searches, annotate text, or navigate through different sections seamlessly. This deficiency can result in user frustration and disengagement, particularly when users seek more interactive functionalities. Consequently, addressing the issue of PDF interaction within library systems emerges as a pivotal endeavor. The enhancement of user engagement and satisfaction relies on the provision of more interactive PDF experiences. Libraries can significantly elevate the user experience by offering interactive PDFs, thereby fostering a greater likelihood of users returning to the library as a valuable and user-centric resource.

A promising solution to the aforementioned challenge is the introduction of the P-BOT, an acronym denoting the "Personalized PDF Chat-BOT." The P-BOT represents an innovative online software platform that leverages the formidable capabilities of the ChatGPT API, offering users a more intuitive and seamless method for engaging with PDF documents. Through seamless integration of the P-BOT into library systems, patrons gain access to a chat-based interface that enables efficient and natural interactions with digital resources, including books, research papers, theses, manuals, essays, and a diverse range of academic content. P-BOT empowers users to pose queries, request assistance, and effortlessly navigate through PDF documents, thus serving as a valuable and transformative addition to library systems. This cutting-edge tool not only enhances user experiences but also streamlines the accessibility and usability of library resources, aligning perfectly with the evolving needs of modern library patrons, researchers, and scholars, thereby contributing to the advancement of knowledge dissemination. P-BOT will incorporate the feature of uploading multiple PDFs, including PDFs of substantial length, such as those spanning up to 1,000 pages.






\section{Literature Survey}

\subsection{History}

Understanding the landscape of generative AI models is crucial for making informed choices regarding their suitability for various applications within our software platform. The foundations of generative models trace back to the 1950s when pioneering efforts led to the development of Hidden Markov Models (HMMs)[5,6] and Gaussian Mixture Models (GMMs)[7]. These early models exhibited the capability to generate sequential data, a milestone achievement with notable applications in speech recognition and time series analysis. However, it was not until the advent of deep learning in more recent years that generative models truly began to shine. Deep learning models, including neural networks, Recurrent Neural Networks (RNNs), and Convolutional Neural Networks (CNNs), have played a pivotal role in significantly enhancing the performance and versatility of generative AI. Their ability to learn complex patterns, capture long-term dependencies, and generate high-dimensional data has revolutionized the field.


Among the earliest methods for sentence generation, N-gram language modeling[8-10] stands as a foundational approach in the realm of Natural Language Processing (NLP). N-grams represent contiguous sequences of words, symbols, or tokens in textual or speech data, defined as neighboring sequences within a given document. These N-grams are integral to a wide array of applications in NLP, extending their influence to domains such as statistical natural language processing, speech recognition, phonemes, machine translation, predictive text input, and more, where the modeling inputs rely on N-gram distributions. N-grams themselves manifest as co-occurring words or items within a specific window, advanced by a fixed number of words or tokens, often referred to as "k." These co-occurring words are termed "n-grams," with "n" signifying the length of the word string considered in constructing these n-grams. The spectrum encompasses unigrams (single words), bigrams (two-word sequences), trigrams (three-word sequences), 4-grams, 5-grams, and so forth. N-grams are harnessed extensively in NLP, text mining, and various natural language processing tasks. Language model development in NLP, for instance, leverages not just unigram models but extends to bigram and trigram models. Major tech companies, including Google and Microsoft, have ventured into constructing expansive web-scale n-gram models, which find utility in NLP tasks like spelling correction, word segmentation, and text summarization. Furthermore, N-grams are vital in crafting features for supervised Machine Learning models, such as Support Vector Machines (SVMs), Maximum Entropy (MaxEnt) models, and Naive Bayes classifiers. The fundamental concept lies in enriching the feature space with tokens like bigrams, trigrams, and more advanced n-grams, transcending the limitations of relying solely on unigrams.


While N-gram language modeling has been instrumental in text generation, its effectiveness is often limited to shorter sentences. In response to this constraint, recurrent neural networks (RNNs) emerged as a pivotal development in the realm of language modeling tasks. RNNs introduced the capacity to model longer dependencies, thereby facilitating the generation of extended and more complex sentences. Subsequently, the introduction of Long Short-Term Memory (LSTM) networks and Gated Recurrent Unit (GRU) models marked a substantial leap forward. These advanced architectures leveraged a gating mechanism to effectively regulate memory during the training process, thereby enabling them to handle sequences of considerably greater length. The capabilities of LSTM and GRU models, which can effectively attend to around 200 tokens, extended the horizons of what could be achieved in language modeling and text generation tasks.

Deep neural networks have had a significant impact on natural language processing. Two key DNN architectures, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are commonly used for NLP tasks.[XX] CNNs excel at capturing position-invariant features, while RNNs are effective at modeling sequences. The competition between CNNs and RNNs often drives advancements in the state-of-the-art for various NLP tasks. But for today, CNNs might be considered less suitable for certain applications as CNNs are not inherently designed for sequential data processing. In NLP, sequences of words are common, and processing them effectively with CNNs can be challenging. RNNs on the other hand,  are a class of neural networks designed to process sequences of data. In a standard Recurrent Neural Network (RNN), the output at each step relies on both the current input and the internal memory or hidden state from the previous step. This mechanism enables RNNs to retain information about past inputs and effectively capture sequential relationships in data.


 Long Short-Term Memory networks (LSTMs) [XX] are a type of RNN that address the vanishing gradient problem by introducing a more sophisticated memory cell. They are designed to capture long-term dependencies in sequences. This approach is designed to address issues related to backflow errors and minimize time intervals. Unlike prior methods, it rapidly learns to differentiate between multiple occurrences of a specific element in an input sequence, even when those occurrences are far apart, without relying on specific short-time-lag training examples but the problem for lstms are they are really hard to train due to very long gradient paths. LSTMs are susceptible to overfitting, especially when dealing with small datasets. They also can be memory-intensive, especially when dealing with very long sequences or complex models.


To mitigate with all the limitations of CNNs and RNNs, the Transformer model was introduced[Transformers: State-of-the-Art Natural Language Processing and attention is all you need]. The Transformers library is an open-source platform designed to assist Transformer-based model architectures and streamline the sharing of pretrained models. Within this library, each model consists of three key components: a tokenizer, a transformer, and a head. Its primary objective is to simplify the usage and dissemination of pretrained models. This library provides a central model hub where various pretrained models can be accessed and utilized, enabling users to compare different models using a consistent API and experiment with shared models across a range of tasks. The Transformers library is a collaborative effort maintained by the Hugging Face research team, with contributions from external researchers and developers. The Transformer model is a groundbreaking encoder-decoder model that doesn't rely on recurrent connections but instead uses attention mechanisms to understand how input and output elements relate to each other. This design allows for much greater parallelization and can achieve state-of-the-art translation quality while requiring considerably less training time. The modles innovation lies in its ability to capture long-range dependencies in data, handle variable-length sequences, and enable parallelization, all of which contribute to its remarkable performance in NLP tasks. The attention mechanism is a valuable tool for focusing on essential information within text data, effectively handling large volumes of information[A Survey on Attention mechanism in NLP]. It utilizes Back Propagation algorithms for classification gradient calculations. This mechanism can be applied in multiple ways, including assessing word importance in input sequences and categorizing the resulting vector through weighted summation. Furthermore, attention serves to enhance memory in Recurrent Neural Network (RNN) models. Notably, models like BERT, built on self-attention, are trained on vast text corpora to predict the subsequent sentence, demonstrating the power of attention mechanisms in various applications.

Bidirectional Encoder Representations from Transformers (BERT) introduced in 2018, is designed to pre-train comprehensive, bidirectional word understandings from unannotated text. It accomplishes this by considering both left and right context in all its layers. An exceptional feature of BERT is its adaptability—it can be fine-tuned for various tasks, like question answering and language inference, with minor adjustments, typically the addition of a single output layer. This flexibility enables BERT to consistently achieve top-tier results in diverse applications without extensive task-specific architectural changes. BERT's pre-training comprises two primary tasks: Masked Language Model (MLM), where it predicts masked words based on surrounding context, and Next Sentence Prediction (NSP), which assesses if one sentence logically follows another in a document. Following pre-training, BERT can be tailored for specific downstream NLP tasks, such as text classification, named entity recognition, and sentiment analysis. Various BERT models, including "BERT-Base" and "BERT-Large," have emerged to cater to different needs. BERT has excelled in numerous NLP applications, serving as a cornerstone for advancements in the field, from question answering and text summarization to machine translation and sentiment analysis.

After a few years, it was found that BERT was significantly undertrained and an improved recipe was proposed for training BERT models, called RoBERTa, that can match or exceed the performance of all of the post-BERT methods.[RoBERTa: A Robustly Optimized BERT Pretraining Approach] RoBERTa builds upon the BERT model and fine-tunes its training methodology to achieve even better performance on various natural language understanding tasks. Modifications to the model are relatively straightforward. Firstly, A longer training duration with the use of larger batches, along with an increase in the volume of training data. Secondly, removal the next sentence prediction objective from the training process. Thirdly, broadening the scope of training by including longer sequences in the training data. Lastly, introduction of dynamic changes to the masking pattern applied to the training data. These adjustments are aimed at enhancing the model's training efficiency and overall performance.

\section{Proposed work}

\subsection{Extraction of Data}

The initial phase in the development of a conversational PDF chatbot involves text extraction from the PDF document. Multiple established technologies are available for this purpose, enabling the extraction of text from searchable PDFs. Notable options among these technologies include PyMuPDF, Pdfminer.six, and PyPdf2.

Based on the findings of a comparative analysis, each of these text extraction methods demonstrated a commendable level of accuracy within a relatively short timeframe. Notably, PyMuPDF outperformed the others, delivering the most precise results. Conversely, PyPdf2 yielded the output with the highest Levenshtein distance, tf-idf, and cosine similarity scores in comparison to the other two methods, making it a less favorable choice.

Both PyMuPDF and pdfminer.six exhibited similar tf-idf and cosine similarity scores, with PyMuPDF having a slightly higher Levenshtein distance than pdfminer.six. However, it's important to note that pdfminer.six required significantly more time to complete the extraction process when compared to PyMuPDF. Consequently, PyMuPDF emerges as the optimal choice for text extraction from PDF documents.

\subsection{Processing of Data}

The extraction of raw text from PDFs necessitates careful processing for effective storage and semantic search. An integral component of this process involves text chunking, a technique used to divide large text segments into smaller, more manageable units. Our examination highlights the impact of various chunking methods, including the NLTK Sentence Tokenizer, Spacy Sentence Splitter, and Langchain Character Text Splitter. Comparative analysis reveals that NLTK and Spacy consistently produce smaller, more digestible sentence segments, whereas Langchain generates larger and denser clusters of text.

Following the chunking process, the text segments are transformed into vector embeddings, numerical representations that capture word and sentence relationships. These embeddings are paramount for efficient storage and to improve semantic searches. By clustering related data points, they streamline the search process. Additionally, we investigate two distinct methods for generating these embeddings: OpenAI embeddings and Instructor embeddings. While OpenAI embeddings are known for their speed and accessibility via APIs, they come at a cost. In contrast, Instructor embeddings offer a budget-friendly alternative but exhibit slightly slower processing times.

\subsection{Storage of Data}

These embeddings are meticulously stored within specialized data repositories, commonly referred to as vector data stores. The vector data store is instrumental in streamlining the organization and retrieval of related data points, endowing LLMs with the capability to swiftly access and retrieve information. Several advanced technologies have emerged to facilitate the creation and utilization of vector data stores, including Facebook AI Similarity Search (FAISS), Pinecone, and Chroma. 


\section{Prepare Your Paper Before Styling}

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{P-BOT: Your personal PDF chatbot\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Shubhi Parashar}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Manan Salia}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
This document is a model and instructions for \LaTeX.
Please observe the conference page limits. 

\section{Literature Review}

\subsection{Maintaining the Integrity of the Specifications}

Deep neural networks have had a significant impact on natural language processing. Two key DNN architectures, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are commonly used for NLP tasks.[XX] CNNs excel at capturing position-invariant features, while RNNs are effective at modeling sequences. The competition between CNNs and RNNs often drives advancements in the state-of-the-art for various NLP tasks. They excel in NLP by treating text as one-dimensional data, allowing them to capture local patterns, n-grams, and features in tasks like text classification, semantic similarity, named entity recognition, document classification, question answering, and text generation. CNNs provide an alternative to recurrent neural networks, offering efficient and effective text modeling, often leveraging pre-trained embeddings and architectural innovations. But for today, CNNs might be considered less suitable for certain applications as CNNs are not inherently designed for sequential data processing. In NLP, sequences of words are common, and processing them effectively with CNNs can be challenging.

 RNNs on the other hand,  are a class of neural networks designed to process sequences of data. In a standard Recurrent Neural Network (RNN), the output at each step relies on both the current input and the internal memory or hidden state from the previous step. This mechanism enables RNNs to retain information about past inputs and effectively capture sequential relationships in data. 

 Long Short-Term Memory networks (LSTMs) [XX] are a type of RNN that address the vanishing gradient problem by introducing a more sophisticated memory cell. The vanishing gradient problem is a difficulty that emerges in the training of deep neural networks, especially in recurrent neural networks (RNNs) and certain deep feedforward networks.[The Vanising Gradient Problem during recurrent neural nets and problems solutions] This issue occurs when the gradients of the loss function concerning the network's parameters become extremely small, almost approaching zero, as they propagate backward through the network's layers during training.
 They are designed to capture long-term dependencies in sequences. This approach is designed to address issues related to backflow errors and minimize time intervals. Unlike prior methods, it rapidly learns to differentiate between multiple occurrences of a specific element in an input sequence, even when those occurrences are far apart, without relying on specific short-time-lag training examples.LSTM networks can effectively capture and connect information across substantial time intervals, even surpassing 1000 discrete-time steps. This ability is achieved by maintaining a consistent flow of error information through dedicated units known as "constant error carousels." Additionally, LSTM employs multiplicative gate units that learn to regulate the access to this constant error flow by determining when to open or close it. 
But the problem for lstms are they are really hard to train due to very long gradient paths. LSTMs are susceptible to overfitting, especially when dealing with small datasets and LSTMs require a significant amount of training data to perform well, and they may not generalize effectively if data is limited. They also can be memory-intensive, especially when dealing with very long sequences or complex models. LSTMs are frequently regarded as inscrutable models, rendering it challenging to understand the decision-making process behind their predictions. This can be a significant concern in situations where transparency and interpretability are of utmost importance.

In order to generate similar sentencse using LSTMs, a new approach was introduced, the Syntactic and Semantic LSTM (SSLSTM)[Generating and Measuring Similar Sentences Using Long Short-Term Memory and Generative Adversarial Networks.], which uses both syntactic and semantic information from sentences to measure their similarity. The central element of SSLSTM is its sentence model, which utilizes sentence-syntactic and word-semantic features to represent the essential language characteristics of input sentences. After segmenting the text into words with word embeddings, CoreNLP is used to generate related sentences by extracting syntactic word relationships. Furthermore, an offline WordNet 3 package is employed to identify similar nouns in sentences and simplify their overall complexity.

Generative Adversarial Networks (GANs) are a class of machine learning models consisting of two neural networks, the generator and the discriminator, engaged in a competitive training process.[Generative Adversarial Networks: Introduction and Outlook] The generator generates artificial data, while the discriminator distinguishes between real and generated data. Through an adversarial interplay, GANs learn to produce high-quality data that is indistinguishable from real data. GANs have been pivotal in tasks like image synthesis, data augmentation, style transfer, and text generation, driving advancements in generative modeling and creative applications in artificial intelligence. While GANs have successfully addressed numerous challenges in generative modeling and inspired other AI techniques, they are not without their limitations. GANs require a careful balance and synchronization between their two adversarial networks during training, a challenging task that can lead to training instability. They share the common issue of poor interpretability with other neural network-based generative models. Additionally, GANs can encounter the "mode collapse" problem, where the generated images often share similar color or texture themes, resulting in limited diversity in the output.

The Gated Recurrent Unit (GRU) is a type of RNN that has demonstrated outstanding performance in Natural Language Processing. [A Multi-scale Convolutional Attention Based GRU Network for Text Classification] GRUs, like other RNNs, are tailored for processing sequential data, which makes them highly suitable for NLP and time-series applications. They employ a gating mechanism akin to LSTMs but offer a relatively simpler architectural structure. GRU was introduced with the inclusion of reset and update gates to capture dependencies at varying time scales. In contrast to the LSTM, the GRU directly exposes its hidden state and doesn't employ an output gate for control. This streamlined architecture reduces the number of parameters in the GRU, making it more straightforward to converge during the training process. Both LSTM and GRU have found extensive application in text classification tasks. GRUs do not possess dedicated memory cells, making them less adept at tasks that require meticulous memory management compared to LSTMs. GRUs have a reduced set of gating mechanisms relative to LSTMs, which might hinder their ability to finely control information flow.



To mitigate with all the limitations of CNNs and RNNs, the Transformer model was introduced[attention is all you need].The Transformer model is a groundbreaking encoder-decoder model that doesn't rely on recurrent connections but instead uses attention mechanisms to understand how input and output elements relate to each other. This design allows for much greater parallelization and can achieve state-of-the-art translation quality while requiring considerably less training time. The modles innovation lies in its ability to capture long-range dependencies in data, handle variable-length sequences, and enable parallelization, all of which contribute to its remarkable performance in NLP tasks. 

The Transformers library is an open-source platform designed to assist Transformer-based model architectures and streamline the sharing of pretrained models.[Transformers: State-of-the-Art Natural Language Processing and ] Within this library, each model consists of three key components: a tokenizer, a transformer, and a head. Its primary objective is to simplify the usage and dissemination of pretrained models. This library provides a central model hub where various pretrained models can be accessed and utilized, enabling users to compare different models using a consistent API and experiment with shared models across a range of tasks. The Transformers library is a collaborative effort maintained by the Hugging Face research team, with contributions from external researchers and developers. 

The attention mechanism is a valuable tool for focusing on essential information within text data, effectively handling large volumes of information[A Survey on Attention mechanism in NLP]. It utilizes Back Propagation algorithms for classification gradient calculations. This mechanism can be applied in multiple ways, including assessing word importance in input sequences and categorizing the resulting vector through weighted summation. Furthermore, attention serves to enhance memory in Recurrent Neural Network (RNN) models. Notably, models like BERT, built on self-attention, are trained on vast text corpora to predict the subsequent sentence, demonstrating the power of attention mechanisms in various applications.

Bidirectional Encoder Representations from Transformers (BERT) introduced in 2018, is designed to pre-train comprehensive, bidirectional word understandings from unannotated text. It accomplishes this by considering both left and right context in all its layers. An exceptional feature of BERT is its adaptability—it can be fine-tuned for various tasks, like question answering and language inference, with minor adjustments, typically the addition of a single output layer. This flexibility enables BERT to consistently achieve top-tier results in diverse applications without extensive task-specific architectural changes. BERT's pre-training comprises two primary tasks: Masked Language Model (MLM), where it predicts masked words based on surrounding context, and Next Sentence Prediction (NSP), which assesses if one sentence logically follows another in a document. Following pre-training, BERT can be tailored for specific downstream NLP tasks, such as text classification, named entity recognition, and sentiment analysis. Various BERT models, including "BERT-Base" and "BERT-Large," have emerged to cater to different needs. BERT has excelled in numerous NLP applications, serving as a cornerstone for advancements in the field, from question answering and text summarization to machine translation and sentiment analysis.
To address the challenge of overfitting when working with limited datasets, a technique called Self-supervised Attention (SSA) was introduced[Improving BERT With Self-Supervised Attention
 and attention is all you need] as well as a co-training framework to optimize the target task. In the auxiliary task, a new sentence is generated based on an original sentence by randomly masking several tokens. The level of randomness decreases with higher levels of generation, resulting in a smoother, less noisy sentence. SSA plays a role in this smoothing process by assigning task-specific weights to each word, contributing to better performance in the auxiliary task.

After a few years, it was found that BERT was significantly undertrained and an improved recipe was proposed for training BERT models, called RoBERTa, that can match or exceed the performance of all of the post-BERT methods.[RoBERTa: A Robustly Optimized BERT Pretraining Approach] RoBERTa builds upon the BERT model and fine-tunes its training methodology to achieve even better performance on various natural language understanding tasks. Modifications to the model are relatively straightforward. Firstly, A longer training duration with the use of larger batches, along with an increase in the volume of training data. Secondly, removal the next sentence prediction objective from the training process. Thirdly, broadening the scope of training by including longer sequences in the training data. Lastly, introduction of dynamic changes to the masking pattern applied to the training data. These adjustments are aimed at enhancing the model's training efficiency and overall performance.

TEXT EXTRACTION

During the late 1990s, when the usage of PDF documents was on the rise, there were initial efforts to extract text from PDFs. These early text extraction techniques typically revolved around dissecting the PDF structure to identify and retrieve text content. Nevertheless, these methods had certain limitations, particularly in terms of accuracy and their ability to preserve document formatting.

OCR, short for Optical Character Recognition[Historical Review of OCR Research and Development], was introduced to meet the critical need for converting printed and handwritten text into digital format. Its primary goal was to facilitate the transition from paper-based content to the digital world, enhancing accessibility, editability, and searchability. OCR not only automated data entry, reducing errors and costs, but also streamlined information retrieval from scanned documents and archives.This technology facilitates the extraction of text from tangible documents, images, and PDFs, ultimately rendering the content searchable, editable, and available for diverse applications. Furthermore, it played a pivotal role in enhancing text accessibility for individuals with visual impairments through its ability to convert text into a readable format for assistive technologies. This technology's introduction represented a groundbreaking transformation in how we manage text-based information, acting as a vital link between the analog and digital realms.

Tesseract OCR, often referred to simply as Tesseract, is an open-source Optical Character Recognition engine created by Google. It has gained recognition for its exceptional precision in identifying printed text within scanned documents, images, and PDFs. Tesseract offers compatibility with a wide array of languages and is capable of extracting text from diverse fonts and text sizes. Its adaptability and ongoing enhancements have solidified its status as a favored solution for tasks such as automating data entry, digitizing printed resources, enabling text searches within images, and supporting a variety of applications that necessitate the transformation of visual text into machine-readable text.

Machine learning (ML) and deep learning have surpassed traditional OCR technology due to their adaptability, superior accuracy, and contextual comprehension of text. Unlike OCR, which often operates within rigid constraints of predefined formats and fonts, ML and deep learning models are highly versatile, capable of accommodating a wide range of document layouts, languages, and even handwritten content. This versatility allows them to extract text with greater precision, making them well-suited for handling complex documents. Furthermore, these models can continually enhance their performance by fine-tuning with new data, ensuring their ongoing relevance and effectiveness in an ever-evolving digital landscape.

The utilization of machine learning and deep learning methods, which encompass natural language processing (NLP) models, has resulted in substantial enhancements in the process of extracting text from PDFs. These advanced models are capable of identifying and extracting text content from intricate PDF documents, including those with multiple columns, tables, and a wide variety of fonts.


PROCESSING OF DATA



\section{Prepare Your Paper Before Styling}
Before you begin to format your paper, first write and save the content as a 
separate text file. Complete all content and organizational editing before 
formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
proofreading, spelling and grammar.

Keep your text and graphic files separate until after the text has been 
formatted and styled. Do not number text heads---{\LaTeX} will do that 
for you.

\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} Yin, Wenpeng, et al. "Comparative study of CNN and RNN for natural language processing." arXiv preprint arXiv:1702.01923 (2017).
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}